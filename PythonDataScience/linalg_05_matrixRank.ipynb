{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86298cc9-e9a4-4d7f-a432-1d4ce22b9109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Python Example\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Example matrix A\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "# Compute the rank of matrix A\n",
    "rank_A = np.linalg.matrix_rank(A)\n",
    "\n",
    "rank_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b14201a7-848e-4619-8d1e-8165ee2ac637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example matrix A\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [8, 8, 9]])\n",
    "\n",
    "# Compute the rank of matrix A\n",
    "rank_A = np.linalg.matrix_rank(A)\n",
    "\n",
    "rank_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab5a6f-b2c3-4b2a-a2f2-7fe6bbb5729f",
   "metadata": {},
   "source": [
    "# Rank of a Matrix\n",
    "\n",
    "The rank of a matrix is a fundamental concept in linear algebra that measures the `maximum number of linearly independent rows or columns` in the matrix. It provides crucial information about the matrix's properties, such as the solutions to a system of linear equations and the dimensionality of the vector space spanned by its rows or columns.\n",
    "\n",
    "## Definition and Intuition\n",
    "- **Rank**: The rank of a matrix \\( A \\) is the dimension of the vector space spanned by its rows (row rank) or columns (column rank). In other words, it is the maximum number of linearly independent row vectors or column vectors in the matrix.\n",
    "- **Linearly Independent**: Vectors are linearly independent if no vector in the set can be written as a linear combination of the others.\n",
    "\n",
    "## Key Properties\n",
    "- **Row Rank Equals Column Rank**: For any matrix, the row rank and column rank are always equal. This common value is simply referred to as the rank of the matrix.\n",
    "- **Full Rank**: A matrix is said to have full rank if its rank is equal to the smaller of the number of rows or columns. For an \\( m \\times n \\) matrix:\n",
    "  - If $ \\text{rank}(A) = m $, the matrix has full row rank.\n",
    "  - If $ \\text{rank}(A) = n $, the matrix has full column rank.\n",
    "\n",
    "## Example\n",
    "\n",
    "Consider the following matrix \\( A \\):\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "- The rows of this matrix are not all linearly independent. In fact, the third row is a linear combination of the first two rows (specifically, the third row is the sum of the first and second rows).\n",
    "- Therefore, the rank of \\( A \\) is 2, not 3, because only two rows (or columns) are linearly independent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562455a1-328a-41f7-a91c-1002172d74ee",
   "metadata": {},
   "source": [
    "Checking Linear Dependence:\n",
    "A row is linearly dependent if it can be written as a sum or multiple of other rows.\n",
    "\n",
    "Looking at the sum of Row 1 and Row 2:\n",
    "\n",
    "(1,2,3)+(4,5,6)=(5,7,9)\n",
    "This is NOT equal to Row 3, which is \n",
    "(7,8,9).\n",
    "\n",
    "Instead, let's check if Row 3 is a linear combination of Row 1 and Row 2:\n",
    "\n",
    "We assume:\n",
    "\n",
    "a⋅(1,2,3)+b⋅(4,5,6)=(7,8,9)\n",
    "This gives three equations:\n",
    "a(1)+b(4)=7\n",
    "a(2)+b(5)=8\n",
    "a(3)+b(6)=9\n",
    "Solving for a and b\n",
    "\n",
    "a+4b=7\n",
    "2a+5b=8\n",
    "Multiply the first equation by 2:\n",
    "2a+8b=14\n",
    "Subtract from the second equation:\n",
    "(2a+5b)−(2a+8b)=8−14\n",
    "−3b=−6⇒b=2\n",
    "Substitute \n",
    "b=2 into the first equation:\n",
    "a=−1\n",
    "Final Equation:\n",
    "\n",
    "(−1)⋅(1,2,3)+(2)⋅(4,5,6)=(7,8,9)\n",
    "\n",
    "(−1,−2,−3)+(8,10,12)=(7,8,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0f9e24-c216-4d9e-be0c-2d57765afbc6",
   "metadata": {},
   "source": [
    "The rank of a matrix plays a significant role in various aspects of data science (DS), machine learning (ML), and deep learning (DL). Here’s why it’s relevant:\n",
    "\n",
    "## 1. Data Dimensionality and Redundancy\n",
    "- **Dimensionality Reduction**: In high-dimensional datasets, not all features may be necessary. The rank of the data matrix can help identify the intrinsic dimensionality, guiding techniques like Principal Component Analysis (PCA) to reduce dimensionality while retaining most of the variance in the data.\n",
    "- **Feature Redundancy**: If the rank of a feature matrix is lower than the number of features, some features are linearly dependent on others. This redundancy can be detected and reduced, leading to more efficient models.\n",
    "\n",
    "## 2. Solving Systems of Linear Equations\n",
    "- **Model Parameters**: Many ML models, such as linear regression, involve solving systems of linear equations. The rank of the coefficient matrix determines if the system has a unique solution, no solution, or infinitely many solutions. A full-rank matrix ensures a unique solution, which is critical for finding optimal model parameters.\n",
    "- **Underfitting and Overfitting**: In cases where the matrix is rank-deficient (not full rank), the model might suffer from underfitting due to lack of sufficient information, or overfitting if the system has too many potential solutions.\n",
    "\n",
    "## 3. Singular Value Decomposition (SVD)\n",
    "- **Latent Factors in Recommender Systems**: SVD is widely used in recommender systems to decompose user-item matrices into latent factors. The rank of the matrix determines the number of latent factors, which helps in making accurate recommendations.\n",
    "- **Noise Reduction**: In DL, SVD can be used for model compression and noise reduction by approximating the weight matrices with lower-rank matrices, which helps in improving generalization.\n",
    "\n",
    "## 4. Matrix Factorization Techniques\n",
    "- **Collaborative Filtering**: Matrix factorization techniques, such as those used in collaborative filtering, rely on the rank of the interaction matrix to identify underlying patterns in user-item interactions. A low-rank approximation can reveal latent structures that drive user preferences.\n",
    "\n",
    "## 5. Training Deep Neural Networks\n",
    "- **Weight Matrices**: In deep learning, the rank of weight matrices in neural networks can affect the capacity of the network to learn complex patterns. Full-rank matrices allow the network to capture a diverse set of features, while rank-deficient matrices may limit the model's expressiveness.\n",
    "- **Gradient-Based Optimization**: During training, ensuring that certain matrices (like the Hessian matrix in second-order optimization methods) have full rank is crucial for stable and efficient learning.\n",
    "\n",
    "## 6. Regularization and Generalization\n",
    "- **Low-Rank Regularization**: Techniques like low-rank regularization are used to enforce sparsity and simplicity in models by penalizing high-rank matrices. This helps in preventing overfitting, leading to better generalization on unseen data.\n",
    "\n",
    "## 7. Kernel Methods\n",
    "- **Support Vector Machines (SVM)**: In SVMs and other kernel-based methods, the rank of the kernel matrix can influence the separation of data points in the feature space. A low-rank kernel matrix may indicate redundancy or insufficient separation between classes.\n",
    "\n",
    "## Summary\n",
    "The rank of a matrix is crucial in DS/ML/DL for understanding the structure and dimensionality of data, ensuring stable and unique solutions to optimization problems, reducing overfitting, and enhancing model performance. It underpins key techniques like PCA, SVD, and matrix factorization, which are fundamental to modern data analysis and machine learning applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a0a97f-e222-485d-a56a-4b62d1d55840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
